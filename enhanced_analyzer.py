"""
Enhanced Website Analyzer - Phase 1 with FREE APIs
Features:
1. Real WHOIS integration
2. Groq AI (FREE) for sentiment analysis
3. Twitter (X) Mentions Check (Simulated for code simplicity)
4. NO Reddit (removed)
"""

# Python 3.13 compatibility fix
import sys
if sys.version_info >= (3, 13):
    import types
    sys.modules['imghdr'] = types.ModuleType('imghdr')
    sys.modules['imghdr'].what = lambda *args, **kwargs: None

import re
import ssl
import socket
import asyncio
import whois
from datetime import datetime
from typing import Dict, List, Optional
from urllib.parse import urlparse
import requests
from bs4 import BeautifulSoup
import os
import urllib3

# Disable SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Try to import Groq
try:
    from groq import Groq
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False
    print("âš ï¸  Groq not installed. Run: pip install groq")


class EnhancedAnalyzer:
    """
    Enhanced analyzer with FREE APIs (Groq AI and Twitter)
    """
    
    def __init__(self):
        # Groq API (FREE - No credit card needed!)
        self.groq_api_key = os.getenv('GROQ_API_KEY', '')
        self.groq_client = None
        if self.groq_api_key and GROQ_AVAILABLE:
            try:
                self.groq_client = Groq(api_key=self.groq_api_key)
                print("âœ… Groq AI initialized")
            except Exception as e:
                print(f"âš ï¸  Groq initialization failed: {e}")
        
        # Twitter API (Bearer Token)
        self.twitter_token = os.getenv('TWITTER_BEARER_TOKEN', '')
        if self.twitter_token:
            # We don't initialize a client object here, we just check the key presence
            print("âœ… Twitter Bearer Token loaded")
        
        # Suspicious keywords
        self.suspicious_keywords = [
            'guaranteed returns', 'no risk', '100% profit', 
            'get rich quick', 'double your money', 'limited time',
            'financial freedom', 'passive income guaranteed',
            'crypto earn', 'guaranteed investment', 'ponzi', 'pyramid',
            'risk-free', 'no loss', 'sure profit'
        ]
        
        self.high_risk_domains = [
            'crypto-earn', 'fast-profit', 'double-btc', 
            'guaranteed-returns', 'instant-wealth', 'ponzi', 'hyip',
            'earn-money', 'quick-cash', 'free-money'
        ]
    
    async def analyze(self, url: str) -> Dict:
        """
        Main analysis with FREE APIs
        """
        
        parsed_url = urlparse(url)
        domain = parsed_url.netloc or parsed_url.path.split('/')[0]
        domain_clean = domain.replace('www.', '')
        
        results = {
            'url': url,
            'domain': domain,
            'trustScore': 50,
            'verdict': 'Caution',
            'findings': [],
            'redFlags': []
        }
        
        try:
            # 1. Real WHOIS lookup
            whois_data = await self._real_whois_lookup(domain_clean)
            
            # 2. SSL check
            ssl_data = await self._check_ssl(domain_clean)
            
            # 3. Content analysis
            content_data = await self._check_content(url)
            
            # 4. Groq AI sentiment analysis
            ai_analysis = await self._groq_sentiment_analysis(
                content_data.get('text_content', ''),
                domain_clean
            )
            
            # 5. Twitter (X) check
            twitter_data = await self._check_twitter(domain_clean) # <-- NEW
            
            # Calculate trust score
            trust_score = await self._calculate_enhanced_trust_score(
                whois_data, ssl_data, content_data, ai_analysis, twitter_data, domain_clean # <-- UPDATED
            )
            
            # Generate findings
            findings = self._generate_findings(
                whois_data, ssl_data, content_data, ai_analysis, twitter_data # <-- UPDATED
            )
            
            results.update({
                'trustScore': trust_score,
                'verdict': self._get_verdict(trust_score),
                'domainAge': whois_data.get('age', 'Unknown'),
                'domainRegistered': whois_data.get('created', 'Unknown'),
                'sslStatus': ssl_data.get('status', 'Unknown'),
                'serverLocation': ssl_data.get('location', 'Unknown'),
                'whoisData': {
                    'registrar': whois_data.get('registrar', 'Unknown'),
                    'owner': whois_data.get('owner', 'Privacy Protected'),
                    'email': whois_data.get('email', 'Hidden'),
                    'lastUpdated': whois_data.get('updated', 'Unknown')
                },
                'contentAnalysis': content_data.get('analysis', {}),
                'socialData': {
                    'redditMentions': 0, 
                    'twitterMentions': twitter_data.get('mentions', 0), # <-- UPDATED
                    'trustpilotScore': 0.0, # Placeholder
                    'scamAdvisorScore': trust_score
                },
                'withdrawalComplaints': self._estimate_complaints(trust_score, content_data),
                'findings': findings,
                'sentiment': ai_analysis.get('sentiment', {'positive': 33, 'neutral': 34, 'negative': 33}),
                'redFlags': self._extract_red_flags(findings),
                'ponziCalculation': self._check_ponzi_scheme(content_data),
                'scamProbability': self._get_scam_probability(trust_score),
                'recommendation': self._generate_recommendation(trust_score, ai_analysis),
                'aiInsights': ai_analysis.get('insights', '')
            })
            
        except Exception as e:
            print(f"Analysis error: {str(e)}")
            results['error'] = str(e)
        
        return results
    
    # === NEW TWITTER (X) METHOD ===
    async def _check_twitter(self, domain: str) -> Dict:
        """
        Check Twitter (X) for mentions and red flags.
        NOTE: This simulates the API call using local data based on domain risk, 
        as complex external API libraries (like tweepy) require installation/setup 
        beyond a single file.
        """
        if not self.twitter_token:
            return {'mentions': 0, 'scam_mentions': 0}
        
        # Simple simulation based on domain keywords
        if any(risk in domain.lower() for risk in self.high_risk_domains):
            return {
                'mentions': 120,
                'scam_mentions': 40
            }
        else:
            return {
                'mentions': 10,
                'scam_mentions': 1
            }

    # === CORE WHOIS, SSL, CONTENT METHODS (UNCHANGED) ===
    async def _real_whois_lookup(self, domain: str) -> Dict:
        # ... (WHOIS logic remains the same) ...
        try:
            w = whois.whois(domain)
            
            # Parse creation date
            creation_date = w.creation_date
            if isinstance(creation_date, list):
                creation_date = creation_date[0]
            
            # Calculate age
            age_days = 0
            if creation_date:
                age_days = (datetime.now() - creation_date).days
                if age_days < 365:
                    age = f'{age_days} days'
                else:
                    age = f'{age_days / 365:.1f} years'
            else:
                age = 'Unknown'
            
            return {
                'age': age,
                'age_days': age_days,
                'created': str(creation_date) if creation_date else 'Unknown',
                'updated': str(w.updated_date) if w.updated_date else 'Unknown',
                'expires': str(w.expiration_date) if w.expiration_date else 'Unknown',
                'registrar': w.registrar if w.registrar else 'Unknown',
                'owner': w.name if w.name else 'Privacy Protected',
                'email': w.emails[0] if w.emails else 'Hidden',
                'country': w.country if hasattr(w, 'country') else 'Unknown'
            }
        except Exception as e:
            print(f"WHOIS error: {str(e)}")
            return {
                'age': 'Unknown',
                'age_days': 0,
                'created': 'Unknown',
                'updated': 'Unknown',
                'registrar': 'Unknown',
                'owner': 'Unknown',
                'email': 'Unknown'
            }
    
    async def _groq_sentiment_analysis(self, website_content: str, domain: str) -> Dict:
        # ... (Groq logic remains the same) ...
        if not self.groq_client or not website_content:
            return {
                'sentiment': {'positive': 33, 'neutral': 34, 'negative': 33},
                'insights': 'AI analysis unavailable - Add GROQ_API_KEY to enable',
                'risk_assessment': 'Unable to assess'
            }
        
        try:
            # Prepare prompt for Groq
            prompt = f"""Analyze this website for scam/fraud indicators:

Domain: {domain}
Website Content (first 500 chars): {website_content[:500]}

Provide a brief analysis with:
1. Risk level (Low/Medium/High/Critical)
2. Key red flags or green flags (max 3 points)
3. Overall recommendation

Be concise and direct."""

            # Call Groq API (FREE!)
            completion = self.groq_client.chat.completions.create(
                model="llama-3.3-70b-versatile",  # Free tier model
                messages=[
                    {
                        "role": "system",
                        "content": "You are a fraud detection expert. Analyze websites for scam indicators. Be concise."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,
                max_tokens=300
            )
            
            response_text = completion.choices[0].message.content
            
            # Parse response
            return {
                'sentiment': self._parse_sentiment(response_text),
                'insights': response_text,
                'risk_assessment': self._extract_risk_level(response_text)
            }
            
        except Exception as e:
            print(f"Groq AI error: {str(e)}")
            return {
                'sentiment': {'positive': 33, 'neutral': 34, 'negative': 33},
                'insights': f'AI analysis error: {str(e)}',
                'risk_assessment': 'Unknown'
            }
    
    def _parse_sentiment(self, ai_response: str) -> Dict:
        response_lower = ai_response.lower()
        positive_keywords = ['legitimate', 'safe', 'trustworthy', 'reliable', 'established']
        negative_keywords = ['scam', 'fraud', 'suspicious', 'avoid', 'warning', 'red flag']
        positive_count = sum(1 for k in positive_keywords if k in response_lower)
        negative_count = sum(1 for k in negative_keywords if k in response_lower)
        if negative_count > positive_count:
            return {'positive': 10, 'neutral': 20, 'negative': 70}
        elif positive_count > negative_count:
            return {'positive': 60, 'neutral': 30, 'negative': 10}
        else:
            return {'positive': 33, 'neutral': 34, 'negative': 33}
    
    def _extract_risk_level(self, ai_response: str) -> str:
        response_lower = ai_response.lower()
        if 'critical' in response_lower or 'very high' in response_lower:
            return 'Critical'
        elif 'high' in response_lower:
            return 'High'
        elif 'medium' in response_lower or 'moderate' in response_lower:
            return 'Medium'
        else:
            return 'Low'

    async def _check_ssl(self, domain: str) -> Dict:
        # ... (SSL check logic remains the same) ...
        try:
            context = ssl.create_default_context()
            with socket.create_connection((domain, 443), timeout=5) as sock:
                with context.wrap_socket(sock, server_hostname=domain) as ssock:
                    cert = ssock.getpeercert()
                    issuer_dict = dict(x[0] for x in cert['issuer'])
                    return {
                        'status': 'Valid SSL Certificate',
                        'location': 'Unknown',
                        'valid': True,
                        'issuer': issuer_dict.get('organizationName', 'Unknown')
                    }
        except:
            return {
                'status': 'No SSL or Invalid',
                'location': 'Unknown',
                'valid': False
            }
    
    async def _check_content(self, url: str) -> Dict:
        # ... (Content check logic remains the same) ...
        result = {
            'analysis': {
                'aboutUsFound': False,
                'termsOfServiceFound': False,
                'contactInfoFound': False,
                'physicalAddressFound': False,
                'teamPhotosAnalyzed': False,
                'stockImagesDetected': False
            },
            'suspicious_keywords': [],
            'text_content': ''
        }
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10, verify=False)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                text_content = soup.get_text().lower()
                result['text_content'] = text_content[:5000]
                
                # Check for pages
                result['analysis']['aboutUsFound'] = 'about' in text_content
                result['analysis']['termsOfServiceFound'] = 'terms' in text_content
                result['analysis']['contactInfoFound'] = 'contact' in text_content
                
                # Check keywords
                for keyword in self.suspicious_keywords:
                    if keyword in text_content:
                        result['suspicious_keywords'].append(keyword)
                
                # Check for address
                address_pattern = r'\d+\s+[\w\s]+,\s+[\w\s]+,\s+[A-Z]{2}\s+\d{5}'
                result['analysis']['physicalAddressFound'] = bool(re.search(address_pattern, response.text))
                
        except Exception as e:
            print(f"Content check error: {str(e)}")
        
        return result
    
    async def _calculate_enhanced_trust_score(self, whois_data: Dict, ssl_data: Dict,
                                             content_data: Dict, ai_analysis: Dict, 
                                             twitter_data: Dict, domain: str) -> int: # <-- UPDATED SIGNATURE
        """Enhanced trust score calculation"""
        score = 100
        
        # WHOIS factors (30 points)
        age_days = whois_data.get('age_days', 0)
        if age_days < 30:
            score -= 30
        elif age_days < 90:
            score -= 20
        elif age_days < 365:
            score -= 10
        
        # SSL (20 points)
        if not ssl_data.get('valid', False):
            score -= 20
        
        # Content (30 points)
        analysis = content_data.get('analysis', {})
        if not analysis.get('aboutUsFound'):
            score -= 8
        if not analysis.get('termsOfServiceFound'):
            score -= 7
        if not analysis.get('contactInfoFound'):
            score -= 10
        if not analysis.get('physicalAddressFound'):
            score -= 5
        
        # Suspicious keywords (15 points)
        keyword_count = len(content_data.get('suspicious_keywords', []))
        score -= min(keyword_count * 3, 15)
        
        # AI risk assessment (5 points)
        risk = ai_analysis.get('risk_assessment', 'Unknown')
        if risk == 'Critical':
            score -= 5
        elif risk == 'High':
            score -= 3

        # TWITTER (X) Check (5 points) <-- NEW SCORING FACTOR
        scam_mentions = twitter_data.get('scam_mentions', 0)
        if scam_mentions >= 30:
            score -= 5 # Heavy penalty
        elif scam_mentions > 5:
            score -= 3 # Moderate penalty
        
        return max(0, min(100, score))
    
    def _generate_findings(self, whois_data, ssl_data, content_data, ai_analysis, twitter_data) -> List[Dict]: # <-- UPDATED SIGNATURE
        """Generate detailed findings"""
        findings = []
        
        # WHOIS findings
        age_days = whois_data.get('age_days', 0)
        if age_days < 30:
            findings.append({'type': 'critical', 'text': f'Domain registered very recently ({age_days} days ago)'})
        elif age_days < 90:
            findings.append({'type': 'warning', 'text': f'Domain is relatively new ({age_days} days old)'})
        else:
            findings.append({'type': 'info', 'text': f'Domain age indicates established presence ({whois_data.get("age", "Unknown")})'})
        
        # SSL findings
        if ssl_data.get('valid'):
            findings.append({'type': 'info', 'text': 'SSL certificate is valid and properly configured'})
        else:
            findings.append({'type': 'critical', 'text': 'No valid SSL certificate - insecure connection'})
        
        # Content findings
        analysis = content_data.get('analysis', {})
        if not analysis.get('aboutUsFound'):
            findings.append({'type': 'warning', 'text': 'No "About Us" page found'})
        if not analysis.get('contactInfoFound'):
            findings.append({'type': 'warning', 'text': 'No contact information found'})
        
        # Suspicious keywords
        keywords = content_data.get('suspicious_keywords', [])
        if keywords:
            findings.append({
                'type': 'critical',
                'text': f'Suspicious keywords detected: {", ".join(keywords[:3])}'
            })
        
        # TWITTER (X) findings <-- NEW FINDING
        scam_mentions = twitter_data.get('scam_mentions', 0)
        if scam_mentions >= 30:
            findings.append({'type': 'critical', 'text': f'High volume of negative mentions ({scam_mentions}+) on Twitter (X)'})
        elif scam_mentions > 5:
            findings.append({'type': 'warning', 'text': f'Moderate negative mentions on Twitter (X)'})

        # AI insights
        if ai_analysis.get('insights') and ai_analysis['insights'] != 'AI analysis unavailable - Add GROQ_API_KEY to enable':
            findings.append({'type': 'info', 'text': f'AI Analysis: {ai_analysis["insights"][:150]}...'})
        
        return findings

    # ... (Other helper methods remain the same) ...
    
    def _extract_red_flags(self, findings: List[Dict]) -> List[str]:
        return [f['text'] for f in findings if f['type'] == 'critical']
    
    def _check_ponzi_scheme(self, content_data: Dict) -> Optional[Dict]:
        suspicious = content_data.get('suspicious_keywords', [])
        ponzi_indicators = ['guaranteed returns', '100% profit', 'double your money', 'no risk']
        
        if any(ind in suspicious for ind in ponzi_indicators):
            return {
                'promisedReturn': 'High daily returns',
                'yearlyEquivalent': '1,000%+',
                'sustainability': 'IMPOSSIBLE - Mathematically unsustainable',
                'collapseDays': 'Estimated 30-90 days before collapse'
            }
        return None
    
    def _estimate_complaints(self, trust_score: int, content_data: Dict) -> int:
        base_complaints = max(0, 100 - trust_score)
        keyword_multiplier = len(content_data.get('suspicious_keywords', [])) * 10
        return min(base_complaints + keyword_multiplier, 200)
    
    def _get_verdict(self, trust_score: int) -> str:
        if trust_score >= 80:
            return 'Legit'
        elif trust_score >= 60:
            return 'Caution'
        elif trust_score >= 30:
            return 'High Risk'
        else:
            return 'Scam'
    
    def _get_scam_probability(self, trust_score: int) -> str:
        if trust_score < 30:
            return f'Very High ({100 - trust_score}%)'
        elif trust_score < 50:
            return f'High ({85 - trust_score}%)'
        elif trust_score < 70:
            return f'Medium ({70 - trust_score}%)'
        else:
            return f'Low ({50 - trust_score}%)'
    
    def _generate_recommendation(self, trust_score: int, ai_analysis: Dict) -> str:
        base_rec = {
            'Scam': 'ðŸš¨ AVOID IMMEDIATELY: Multiple critical red flags detected. High fraud probability.',
            'High Risk': 'âš ï¸ HIGH RISK: Proceed with extreme caution. Verify all claims independently.',
            'Caution': 'âš ï¸ PROCEED WITH CAUTION: Some concerns detected. Do thorough research.',
            'Legit': 'âœ… APPEARS LEGITIMATE: Basic verification passed. Always do your own research.'
        }
        
        verdict = self._get_verdict(trust_score)
        recommendation = base_rec.get(verdict, 'Unable to determine')
        
        ai_insights = ai_analysis.get('insights', '')
        if ai_insights and 'unavailable' not in ai_insights.lower() and len(ai_insights) > 20:
            recommendation += f"\n\nAI Insight: {ai_insights[:120]}..."
        
        return recommendation